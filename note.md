### Tensorflow skillpath
- Resources:
    - Book: Deep Learning with Python, François Chollet
    - Documentation: Keras Library
    - Documentation: Tensorflow Library
    - Book: Algorithms of Oppression: How Search Engines Reinforce Racism, Safiya Umoja Noble
    - Book: Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, Cathy O’Neil
- what is "deep"? : The deep part of deep learning refers to the numerous “layers” that transform data. This architecture mimics the structure of the brain, where each successive layer attempts to learn progressively complex patterns from the data fed into the model.
- There was a final step in the Perceptron algorithm that would give rise to the incredibly mysterious world of Neural Networks — the artificial neuron could train itself based on its own results, and fire better results in the future. In other words, it could learn by trial and error, just like a biological neuron.
- It was found out that creating multiple layers of neurons — with one layer feeding its output to the next layer as input — could process a wide range of inputs, make complex decisions, and still produce meaningful results. With some tweaks, the algorithm became known as the Multilayer Perceptron, which led to the rise of Feedforward Neural Networks.
- The data structure we use in deep learning is called a **tensor**, which is a generalized form of a vector and matrix: a multidimensional array.
- Neural network has different types of layers:
    - The value for each of these features would be held in an input node.
    - Hidden layers are layers that come between the input layer and the output layer. 
    - Output layer is the final layer in our neural network.
- The weighted sum between nodes and weights is calculated between each layer:
    - weighted_sum=(inputs⋅weight_transpose)+bias
    - Activation(weighted_sum)
    - Activation functions introduce nonlinearity in our learning model, creating more complexity during the learning process.
    - An activation function decides what is fired to the next neuron based on its calculation for the weighted sums.
- Forward propagation: input -> hidden note -> output
    - A bias node shifts the activation function either left or right to create the best fit for the given data in a deep learning model.
- Loss functions:
    - Cross-entropy loss: is used for classification learning models rather than regression.
- Backpropagration: computation of gradients with an algorithm known as gradient descent. This algorithm continuously updates and refines the weights between neurons to minimize our loss function.
- Gradient descent: parameter_new=parameter_old+learning_rate⋅gradient(loss_function(parameter_old))
- Stochastic Gradient Descent (SGD): Instead of performing gradient descent on our entire big dataset, we pick out a random data point to use at each iteration.
- Adam optimization algorithm: an adaptive learning algorithm that finds individual learning rates for each parameter. Ability to have an adaptive learning rate has made it an ideal variant of gradient descent and is commonly used in deep learning models.
- Mini-batch gradient descent is similar to SGD except instead of iterating on one data point at a time, we iterate on small batches of fixed size. Ideal trade-off between GD and SGD. Since mini-batch does not depend on just one training sample, it has a much smoother curve and is less affected by outliers and noisy data making it a more optimal algorithm for gradient descent than SGD.
- [Loss vs epochs among GD-SGD-Adam.jpg]
- Key issues:
    - Machine learning algorithms can only be as good as the data it is trained on
    - Machine learning models do not understand the impact of a false negative vs. a false positive diagnostic (at least not like humans can)
    - For many of the clinicians and the patients, the models are a black box.